\documentclass[a4paper,11pt]{article}

%--------------------------------------------------------------------
% PACKAGES
%--------------------------------------------------------------------
\usepackage{fontspec}
\usepackage[greek,english]{babel} % Greek + English
\usepackage{xcolor}
\usepackage{xgreek}               % For Greek text
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{graphicx}             % For including images
\usepackage{float}                % For positioning images with 'H' option
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{fmtcount}
\usepackage{titlesec}
\usepackage[a4paper, margin=1in]{geometry}

%--------------------------------------------------------------------
% FONTS & LAYOUT ADJUSTMENTS (optional)
%--------------------------------------------------------------------
% If you face issues with Greek in fontspec-based LaTeX compilers (XeLaTeX/LuaLaTeX),
% set a main font that supports Greek.
%\setmainfont{Times New Roman} % or any other that has Greek glyphs
\setmainfont{Times New Roman}
%--------------------------------------------------------------------
% DOCUMENT
%--------------------------------------------------------------------
\begin{document}
\selectlanguage{greek}

\title{\textbf{Αναφορά: Πρόβλεψη Alzheimer’s με Χρήση Νευρωνικών Δικτύων}}
\author{\\
\large Όνομα Επώνυμο \\
\large Α.Μ. φοιτητή: XXXXX \\
\large \texttt{email@upatras.gr}
}
\date{\today}
\maketitle

%--------------------------------------------------------------------
% 1. ΕΙΣΑΓΩΓΗ
%--------------------------------------------------------------------
\section{Εισαγωγή}
Στην παρούσα εργασία μελετάμε τη χρήση Τεχνητών Νευρωνικών Δικτύων (ΤΝΔ) για την
πρόβλεψη της νόσου Alzheimer, χρησιμοποιώντας το σύνολο δεδομένων που παρέχεται
(Alzheimer’s Disease Dataset). Στόχος είναι να ταξινομήσουμε ασθενείς σε δύο κλάσεις
(Alzheimer ή μη) βάσει διαφόρων χαρακτηριστικών (βιοδείκτες, συμπτώματα, κ.λπ.).

Παρακάτω παρουσιάζονται τα βήματα που ακολουθήθηκαν, σύμφωνα με τα ερωτήματα του
Μέρους Α’ της εργασίας.

%--------------------------------------------------------------------
% A1. ΠΡΟΕΠΕΞΕΡΓΑΣΙΑ
%--------------------------------------------------------------------
\section{A1: Προεπεξεργασία και Προετοιμασία Δεδομένων}

\subsection{(α) Κωδικοποίηση και προεπεξεργασία δεδομένων}
Στο σύνολο δεδομένων εντοπίστηκαν κατηγορικές και ποσοτικές μεταβλητές, οπότε:
\begin{itemize}
    \item \textbf{One-hot encoding:} Εφαρμόστηκε στις κατηγορικές μεταβλητές, όπως \emph{φύλο}, \emph{εθνικότητα}, κ.λπ.
    \item \textbf{Κανονικοποίηση/Τυποποίηση:} (Π.χ.) Χρησιμοποιήθηκε η \emph{Min-Max} κανονικοποίηση για να φέρουμε τις τιμές στην κλίμακα [0, 1], μειώνοντας έτσι τυχόν πόλωση στις τιμές. 
    % ή
    % Χρησιμοποιήθηκε η τυποποίηση (z-score) ώστε κάθε χαρακτηριστικό να έχει μέση τιμή 0 και τυπική απόκλιση 1.
\end{itemize}

\noindent
\textbf{Ενδεικτικός ψευδοκώδικας}:
\begin{lstlisting}[language=Python]
# Example: Min-Max Normalization of a single feature X
X_norm = (X - X.min()) / (X.max() - X.min())

# Example: Standardization (Z-score)
X_standard = (X - mean(X)) / std(X)
\end{lstlisting}

\subsection{(β) Διασταυρούμενη Επικύρωση (5-fold CV)}
Για την αξιόπιστη εκτίμηση των επιδόσεων των μοντέλων, χρησιμοποιήθηκε \emph{5-fold
Cross Validation}. Φροντίσαμε τα folds να είναι \emph{ισορροπημένα} (balanced) ως προς
τον αριθμό δειγμάτων ανά κλάση. Σε κάθε πείραμα:
\begin{itemize}
    \item Διαχωρίστηκαν τα δεδομένα σε 5 $\approx$ ίσα τμήματα (folds).
    \item Κάθε φορά, 4 folds χρησιμοποιήθηκαν για εκπαίδευση και 1 για έλεγχο.
    \item Επαναλήφθηκε η διαδικασία 5 φορές και υπολογίστηκε ο μέσος όρος των μετρικών.
\end{itemize}

%--------------------------------------------------------------------
% A2. ΕΠΙΛΟΓΗ ΑΡΧΙΤΕΚΤΟΝΙΚΗΣ
%--------------------------------------------------------------------
\section{A2: Επιλογή Αρχιτεκτονικής}

\subsection{(α) Σημασία των μετρικών CE, MSE, Accuracy}
\begin{itemize}
  \item \textbf{Cross-Entropy (CE) loss:} Καταλληλότερη σε περιπτώσεις ταξινόμησης, καθώς τιμωρεί έντονα σφάλματα στην πιθανότητα της σωστής κλάσης και συνήθως οδηγεί σε καλύτερη σύγκλιση.
  \item \textbf{Μέσο Τετραγωνικό Σφάλμα (MSE):} Χρησιμοποιείται συχνά σε προβλήματα παλινδρόμησης. Σε ταξινόμηση μπορεί να οδηγήσει σε βραδύτερη/όχι ιδανική σύγκλιση.
  \item \textbf{Ακρίβεια (Accuracy):} Αφορά το ποσοστό σωστών προβλέψεων. Είναι πολύ κατανοητή μετρική αξιολόγησης (τελική απόδοση), αλλά δεν είναι συνήθως η βέλτιστη \emph{συνάρτηση κόστους} για εκπαίδευση.
\end{itemize}
\noindent
\textbf{Συμπέρασμα:} Για την εκπαίδευση (loss) χρησιμοποιούμε CE, ενώ για αναφορά
τελικών επιδόσεων χρησιμοποιούμε ακρίβεια.

\subsection{(β) Αριθμός νευρώνων στο επίπεδο εξόδου}
Για μια \emph{binary classification} (Alzheimer ή μη) αρκεί:
\begin{itemize}
    \item 1 νευρώνας εξόδου με σιγμοειδή λειτουργία (παραγωγή $p \in (0,1)$), ή
    \item 2 νευρώνες εξόδου με \emph{Softmax} σε διανυσματική μορφή (πιο γενικευμένη προσέγγιση).
\end{itemize}

\subsection{(γ) Επιλογή συνάρτησης ενεργοποίησης στους κρυφούς κόμβους}
\begin{itemize}
    \item \textbf{Tanh:} Καλύτερη κλίμακα τιμών από τη σιγμοειδή (παράγει τιμές σε $[-1, 1]$), αλλά μπορεί να υποφέρει από vanishing gradients.
    \item \textbf{ReLU:} Συχνά προτιμάται σε βαθιά δίκτυα, έχει γρήγορη σύγκλιση, αλλά υπάρχει η περίπτωση “dying ReLUs”.
    \item \textbf{SiLU (Sigmoid-Weighted Linear Unit, \emph{swish}):} Εναλλακτική που σε ορισμένες περιπτώσεις ξεπερνά σε επίδοση το ReLU, αλλά είναι πιο σύνθετη.
\end{itemize}
\noindent
\textbf{Επιλογή μας}: (Π.χ.) ReLU ή SiLU, βάσει πειραματικής αξιολόγησης.

\subsection{(δ) Συνάρτηση ενεργοποίησης στο επίπεδο εξόδου}
\begin{itemize}
    \item \textbf{Σιγμοειδής}: Χρήσιμη όταν υπάρχει ένας νευρώνας εξόδου, για binary classification (παράγει πιθανότητα).
    \item \textbf{Softmax}: Αν έχουμε 2 νευρώνες, π.χ. για να εξασφαλίζουμε $p_1 + p_2 = 1$.
\end{itemize}

\subsection{(ε) Αριθμός νευρώνων στο κρυφό επίπεδο και αποτελέσματα}
Παρακάτω ο πίνακας με διαφορετικούς αριθμούς κρυφών νευρώνων $H$. Υποθέτουμε ότι $I$ είναι ο αριθμός εισόδων. Ενδεικτικά, ακολουθούν τιμές και γραφήματα σύγκλισης. (Συμπληρώστε με τα πραγματικά αποτελέσματα.)

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Νευρώνες $H$} & \textbf{CE Loss} & \textbf{MSE} & \textbf{Accuracy} \\
\hline
$I/2$   & ... & ... & ... \\
\hline
$2I/3$  & ... & ... & ... \\
\hline
$I$     & ... & ... & ... \\
\hline
$2I$    & ... & ... & ... \\
\hline
\end{tabular}
\end{center}

\noindent
\textbf{Συμπεράσματα:}
\begin{itemize}
    \item \emph{(i)} Κατάλληλος αριθμός κρυφών νευρώνων: \dots
    \item \emph{(ii)} Συνάρτηση κόστους που δίνει βέλτιρη απόδοση: \dots
    \item \emph{(iii)} Συνάρτηση ενεργοποίησης που οδηγεί σε βέλτιρη μάθηση: \dots
    \item \emph{(iv)} Ταχύτητα σύγκλισης / εποχές εκπαίδευσης: \dots
\end{itemize}

\subsection{(στ) Κριτήριο τερματισμού}
Χρησιμοποιήθηκε (π.χ.) \emph{early stopping}:
\begin{itemize}
    \item Παρακολουθούμε την απόδοση στο validation fold.
    \item Αν για $N$ συνεχόμενες εποχές δεν βελτιώνεται η CE ή η ακρίβεια, σταματάμε.
\end{itemize}

%--------------------------------------------------------------------
% A3. ΜΕΤΑΒΟΛΕΣ ΣΕ ΡΥΘΜΟ ΕΚΠΑΙΔΕΥΣΗΣ (η) ΚΑΙ ΣΤΑΘΕΡΑ ΟΡΜΗΣ (m)
%--------------------------------------------------------------------
\section{A3: Μεταβολές στον ρυθμό εκπαίδευσης και στη σταθερά ορμής}
Έπειτα από επιλογή της καλύτερης τοπολογίας στο Α2, πειραματιστήκαμε με διαφορετικές
τιμές $\eta$ και $m$ (learning rate και momentum). Ο πίνακας συνοψίζει τα αποτελέσματα
μετά από 5-fold CV:

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{$\eta$} & \textbf{$m$} & \textbf{CE Loss} & \textbf{MSE} & \textbf{Accuracy} \\
\hline
0.001 & 0.2 & ... & ... & ... \\
\hline
0.001 & 0.6 & ... & ... & ... \\
\hline
0.05  & 0.6 & ... & ... & ... \\
\hline
0.1   & 0.6 & ... & ... & ... \\
\hline
\end{tabular}
\end{center}

\noindent
\textbf{Γιατί $m < 1$:} Θεωρητικά, το momentum όρος $\alpha \cdot \Delta w_{t-1}$ πρέπει να
είναι μικρότερος από το κύριο βήμα ενημέρωσης, ώστε να μην “εκτοξεύεται” η προσαρμογή
βαρών. % Συμπληρώστε όποιες πρόσθετες θεωρητικές λεπτομέρειες

\noindent
\textbf{Συμπεράσματα:} \dots

%--------------------------------------------------------------------
% A4. ΚΑΝΟΝΙΚΟΠΟΙΗΣΗ (REGULARIZATION)
%--------------------------------------------------------------------
\section{A4: Ομαλοποίηση (Regularization)}
Για την αποφυγή υπερπροσαρμογής (overfitting), δοκιμάσαμε L1 ή L2. Συνήθως, \textbf{L2}
(\emph{weight decay}) επιλέγεται για δίκτυα πλήρους συνδεσμολογίας ως πιο “ομαλή”
λύση. Δοκιμάσαμε διάφορες τιμές $r$ (regularization hyperparameter):

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{$r$} & \textbf{CE Loss} & \textbf{MSE} & \textbf{Accuracy} \\
\hline
0.0001 & ... & ... & ... \\
\hline
0.001  & ... & ... & ... \\
\hline
0.01   & ... & ... & ... \\
\hline
\end{tabular}
\end{center}

\noindent
\textbf{Συμπεράσματα:} \dots

%--------------------------------------------------------------------
% A5. ΒΑΘΥ ΝΕΥΡΩΝΙΚΟ ΔΙΚΤΥΟ (ΠΡΟΑΙΡΕΤΙΚΟ)
%--------------------------------------------------------------------
\section{A5: Βαθύ Νευρωνικό Δίκτυο (Προαιρετικό - Bonus 10\%)}
Δοκιμάστηκε η επέκταση σε δύο ή τρία κρυφά επίπεδα. \\
\textbf{Ενδεικτικές τοπολογίες}: 
\begin{itemize}
    \item Δύο κρυφά επίπεδα: $(I) - H_1 - H_2 - (Output)$
    \item Τρία κρυφά επίπεδα: $(I) - H_1 - H_2 - H_3 - (Output)$
\end{itemize}
Συζητήστε αν είναι χρήσιμο να μειώνεται/αυξάνεται ο αριθμός νευρώνων στα διαδοχικά
κρυφά επίπεδα, παρουσιάστε τα αποτελέσματα (CE, MSE, Accuracy) και τα συμπεράσματά σας.

%--------------------------------------------------------------------
% ΣΥΜΠΕΡΑΣΜΑΤΑ
%--------------------------------------------------------------------
\section{Συνολικά Συμπεράσματα}
Παραθέστε μια γενική σύνοψη των ευρημάτων σας, συνοψίζοντας:
\begin{itemize}
    \item Την καλύτερη τοπολογία και υπερπαραμέτρους που επιλέξατε
    \item Την τελική ακρίβεια ταξινόμησης
    \item Τυχόν πρακτικές παρατηρήσεις ή δυσκολίες
\end{itemize}

%--------------------------------------------------------------------
% ΒΙΒΛΙΟΓΡΑΦΙΑ (αν χρειάζεται)
%--------------------------------------------------------------------
%\begin{thebibliography}{99}
%\bibitem{key1}
%  ...
%\end{thebibliography}

\end{document}
